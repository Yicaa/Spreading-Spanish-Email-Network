---
title: "Spreading: Spanish University Email Network"
author: "Yijia Lin and Bradley McKenzie"
date: "`r Sys.Date()`"
output: 
  rmdformats::html_clean:
    lightbox: false
    thumbnails: false
    toc: yes
    toc_float: true
    toc_depth: 3
editor_options: 
  markdown: 
    wrap: 72
---

```{r,include=F}
knitr::opts_chunk$set(cache=T,message=FALSE,warning=FALSE)
options(scipen=700)
```

## Libraries

```{r, message=FALSE, warning=FALSE}
rm(list = ls())
library(tidyverse)
library(igraph)
library(tidygraph)
library(ggplot2)
library(ggraph)
library(ggthemes)
library(here)
library(caret)
library(purrr)
```

## Read the data

```{r, message=FALSE, warning=FALSE}
nodes <- read_csv(here("network_data", "nodes.csv"))
links <- read_csv(here("network_data", "edges.csv"))
```

## Description of the dataset

This network is a directed and unweighted social communication network,
which represents the exchange of emails among members of the Rovira i
Virgili University in Spain, in 2003.

Source: [Netzschleuder](https://networks.skewed.de/net/uni_email).

### Interpreting the email dataset as a SIR model

To help interpret the findings within the context of the university
email network, we explain the following settings can specify the
following settings. In the setting of a university email network We can
consider the epidemic as the spread of rumors between individuals. For
example, it could be some sort of negative campaign about a Professor to
ruin their reputation (but never a Social Networks teacher obviously -
don't worry Blas, you're safe :D).

We could the actors in the model:

-   Susceptible (S): Individuals who have not yet received the email

-   Infected (I): Individuals who have already recieved the bad email
    and joined the campaign. They can now send the rumors on to other
    individuals in the university

-   Recovered (R): Individuals who have already received the bad email
    and were infected, but now realize that it's just a rumor and are
    now not at risk of believing the email if they see it again AND will
    not send it to others if they see it. They are immune to the rumor.

We could consider the email as a SI model if it's

### Understanding this model stype:

In terms of model outcome, we could consider the outcomes as:

Susceptible-Infected (SI): Everyone who received the virus is infected
and loses control of their computer and the virus can be sent on to all
of their connections. (This is the least difficult to justify in this
setting)

Susceptible-Infected-Recovered (SIR): Everyone who received the email
virus has installed proper seucity software and is not as risk of
receiving anymore malware.

Susceptible-Infected-Susceptible (SIS): People have IT make a temporary
fix to remove the email virus, but it is not a long term solution. The
computer does not have immunity.

#### Nodes and links

```{r}
head(nodes)
head(links)
```

Within the `nodes` dataframe, the column `# index` is the one indicating
the index of the nodes, the column `name` would contain the
corresponding name of each node, but they have been anonymized for
privacy reason so this column is still a series of numbers. The `_pos`
column represents the coordinates of each node in a 2D space, used for
visualization or layout in graph-related tasks.

In the `links` dataframe, the column `# source` and `target` indicate
the indexes of the 2 nodes forming a link.

#### Graph

```{r}
graph <- graph_from_data_frame(links, directed = TRUE, vertices = nodes)
graph
```

## Initial graph exploration

To understand the graph, we can plot the degree distribution to check
whether we have the expected power-law distribution or another type.
This will help to interpret the findings later when we remove links.

First, we check for any loops, these would be emails sent to self. And
if so, we can simplify the graph to remove these (and also any eventual
multiple edge).

```{r}
paste0("The number of recursive (self-directed) emails in the network is: ", 
       ecount(graph) - ecount(simplify(graph)))

# we see that one exists, so we simplify the graph to remove it. 
graph <- simplify(graph)

# check our graph is setup properly
is_igraph(graph)
```

Next, calculate basic descriptive statistics of the plot:

```{r}
paste0("There are ", vcount(graph), " nodes and ", ecount(graph), " edges in our Spanish email network")

# check if our graph is fully connected
is_connected(graph)

#degree
degree_all <- degree(graph, mode="all")
degree_out <- degree(graph, mode="out")
paste0("The average degree is: ", round(mean(degree(graph)), 2), " when we DO NOT consider the direction of the emails.")
paste0("The average degree is: ", round(sum(degree_out)/vcount(graph), 2), " when we DO consider the emails to be directed outward from the node (sender) to a recipient")

# most connected node:
paste0("The most connected node is number ", which.max(degree(graph))[1], " which has ", max(degree(graph)), " links.")
```

For the purposes of this homework, we are not factoring in this
directional property of the graph. We are considering that the emails
are indicating a social relationship between the two nodes (sender and
receiver) and it is irrelevant who sent the email. This means the
average interactions for each individual node is 19.24 (which would
represent email interactions with 19.24 different people on average).

Now check the degree distribution;

```{r, fig.width=8}
# set average node for label 
avg_deg <- round(mean(degree(graph)),2)

# plot linear distribution
ggplot() + 
  geom_histogram(aes(x = degree_all), bins=40) + 
  geom_vline(aes(xintercept = avg_deg, colour = "red"), show.legend = FALSE) +
  annotate("text", x = avg_deg+5, y = 160, label = paste0("Avg deg = ", avg_deg), angle = 90, color = "red") + 
  labs(title = "Degree distribution histogram for Uni emails",
       x = "Degrees",
       y = "Number of nodes") +
  theme(axis.title.y = element_text(angle = 90))
```

We see the common power law distribution. There are few very connected
nodes and many less connected nodes. We see that the max node must be
node number 105 we reported earlier, with its 142 links. This is far
more than the second most connected.

Visualization of the network:

```{r, fig.width=8}
# set colour and size only for the top 50 nodes
top_50_nodes <- order(degree_all, decreasing = TRUE)[1:50]
V(graph)$color <- "grey3"
V(graph)[top_50_nodes]$color <- "red"
V(graph)$size <- 0.75
V(graph)[top_50_nodes]$size <- 2

# plot the full network
ggraph(graph, layout = "kk") +
  geom_edge_link(width = 0.1, alpha = 0.5, color = "grey") +  
  geom_node_point(color = V(graph)$color, size = V(graph)$size) +  
  theme_void() +  
  ggtitle("Visualisation of the network, 50 most connected nodes highlighted")+
  theme(legend.position = "none")
```

We see that the most connected nodes are all centered in the graph. On
the outside, we can see the nodes with only a couple of connections.

## Steps

### 1) Find the theoretical epidemic threshold for your network for the information βc to reach a significant number of nodes.

The formula to calculate the theoretical epidemic threshold that we
learned from class is:

$$ \beta_c = \mu \frac{\langle k \rangle}{\langle k^2 \rangle}$$

where:

-   **μ (mu)**: Recovery rate — typically set to **0.1** in SIR models.

-   **⟨k⟩ (mean degree)**: The **average number of connections** (edges)
    per node in the network.

-   **⟨k²⟩ (mean squared degree)**: The **average of the square of each
    node’s degree** — it captures how much variability (or inequality)
    there is in node connectivity.

#### Updating our network to be undirected

Firstly, we make our network undirected. We use this to create a more
true social network where spreading is symmetric between nodes and can
happen either way.

We use the undirected_graph function from igraph with mode="collapse" to
update the graph. The effect of this is:

-   It merges reciprocal edges (A→B and B→A) into one link, which
    matches the idea of a “social tie”.

That’s why with collapse, the number of edges drops by half. this better
reflects the true number of relationships.

```{r}
# Create the undirected version of the graph
graph <- as_undirected(graph, mode = "collapse")

paste0("There are ", vcount(graph), " nodes and ", ecount(graph), " edges in our undirected Spanish email network.")


```

Now we start to calculate epidemic threshold using degree moments:

```{r}
# Set recovery rate (mu) — use the standard 0.1 value
mu <- 0.1

# Calculate degree of each node
k <- degree(graph)

# Mean degree and mean squared degree
mean_k <- mean(k)
mean_k2 <- mean(k^2)

# Calculate threshold with our formula: βc = μ * <k> / <k^2>
beta_c <- mu * mean(degree(graph)) / (mean(degree(graph)^2))

# Output the result
paste0("Theoretical epidemic threshold βc is approximately: ", round(beta_c, 5),
       " because ⟨k²⟩ = ", round(mean_k2, 4), " > ⟨k⟩ = ", round(mean_k, 4))

```

The estimated threshold βc ≈ 0.00535 is **very low**, indicating that
even a small infection rate β would allow the information to reach a
significant portion of the network.

This low threshold is a direct consequence of the high variance in
degree:\

⟨k⟩ = 9.62 but ⟨k²⟩ = 179.82, which suggests the presence of a few nodes
with **very high degree** (i.e., “super spreaders”).

The large gap between ⟨k⟩ and ⟨k²⟩ is a signature of **heterogeneity**
in the network. This is typical in real-world social networks, where
most people have few contacts, and a few individuals are highly
connected.

<br> <br>

### 2) Assuming that randomly-selected 1% initial spreaders, simulate the SIR model below and above that threshold and plot the number of infected people as a function of β.

Here we simulate the SIR model with different β values. In the model,
the values represent:

-   0 = S, 1 = I, 2 = R

The broad process is:

1.  Set the inputs - graph, threshold, recovery rate & seeds.

2.  Initialise all nodes as 0 in period 0, except infected which are the
    randomly assigned seeds.

3.  Create output table with time and infection period

4.  Loop through while there are still infected people. Adding new
    infections to the table in the period they became infected.

All nodes start as value 0 as they're uninfected. We then apply
infection to the randomly selected seeds and

```{r}
# Set recovery rate μ
mu <- 0.1

# SIR simulation function

sim_sir <- function(g, beta, mu, seeds){
  
  state <- rep(0, vcount(g))     
  state[seeds] <- 1              # set initial spreaders
  t <- 0
  
  table <- data.frame(t=0, inf=seeds)
  while(sum(state == 1) > 0){    
    t <- t + 1
    new_inf <- c()
    for(i in which(state == 1)){ # among all infected ones
      neighbors <- neighbors(g, i)
      for(j in neighbors){
        if(state[j] == 0 && runif(1) < beta){  
          # If neighbor j is susceptible (i.e., state[j] == 0), then infect it with probability β. "runif(1) < beta" means to generate a random number between 0 and 1. If it’s less than β, the infection is considered successful.
          new_inf <- c(new_inf, j)
        }
      }
      if(runif(1) < mu){  # Recover an infected node with probability μ
        state[i] <- 2
      }
    }
    new_inf <- unique(new_inf) # fix: avoid duplicates from multiple infected neighbors
    state[new_inf] <- 1
    if(length(new_inf) > 0){  # only record when there is new infection
      table <- rbind(table, data.frame(t=t, inf=new_inf))
    }
  }
  distinct(table) # adding distinct function to keep only unique nodes at each stage, avoiding repeated counts from infections to multiple neighbours in the same time period
}

```

Now we run the simulations over different values. Since our critical
threshold was 0.00535, we select 2 values smaller than the threshold and
2 above.

```{r}

# set the thresholds, β, to test （lower than, close to, or higher than βc）
beta_vals <- c(0, 0.002, 0.005, beta_c, 0.01, 0.02, 0.1)

# keep recovery rate (mu) at 0.1
mu <- 0.1

# randomly set 1% as seeds - this is 12 nodes (of our 1133 total)
set.seed(123)
num_seeds <- ceiling(0.01 * vcount(graph))
seeds <- sample(1:vcount(graph), num_seeds)

# run n times of simulation for each β. Calculate infection total for each. 
n_runs <- 10
results <- data.frame()

for(beta in beta_vals){
  final_counts <- c()
  for(i in 1:n_runs){
    sim <- sim_sir(graph, beta, mu, seeds)
    final_counts <- c(final_counts, length(unique(sim$inf)))
  }
  avg_inf <- mean(final_counts)
  sd_inf <- sd(final_counts)
  results <- rbind(results, data.frame(beta=beta, avg_infected=avg_inf, sd=round(sd_inf,2)))
}
# and view the results:
results
```

Now we can plot the results to compare more easily:

```{r}
# viz：β vs averaged number of infected people
ggplot(results, aes(x=beta, y=avg_infected)) +
  geom_line(color="steelblue") +
  geom_point(size=3) +
  geom_errorbar(aes(ymin=avg_infected - sd, 
                    ymax=avg_infected + sd), 
                width=0.001, alpha=0.6) +
  labs(title = "Infection Size vs β (Random 1% Seeds)",
       x = expression(beta),
       y = "Average number of infected nodes") +
  theme_minimal()
```

The plot clearly demonstrates the presence of an **epidemic threshold**
in the network. When the infection rate β is below approximately
0.00535, the average number of infected nodes remains low, indicating
limited or failed spread. However, just beyond this value, there is a
**sudden and sharp increase** in the number of infections, revealing a
**nonlinear transition** typical of epidemic processes in complex
networks.

Furthermore, the plot shows a **monotonic increase** in the average
infection size as β increases, aligning with theoretical expectations
from the SIR model — higher infection rates naturally result in more
widespread diffusion. Notably, the **uncertainty (standard deviation)**
of infection size is highest around β = 0.01, suggesting that outcomes
near the threshold are highly sensitive to stochastic factors such as
seed selection and local network topology.

Overall, the plot illustrates a classic **epidemic threshold
phenomenon**. Once β exceeds the critical value, even a small increase
in the infection probability can lead to a disproportionately large
outbreak, confirming the nonlinear nature of spreading processes in
heterogeneous networks.

Now we plot the general trend to try identify smoothness (note that this
plot only has 1 run per point so could be considered less robust than
the previous).

```{r}
# set the seeds for consistency in the starting infection nodes:
set.seed(123)
num_seeds <- ceiling(0.01 * vcount(graph))
seeds <- sample(1:vcount(graph), num_seeds)

# plot the outputs over time
results <- map_dfr(seq(0,0.1,0.005), # beta range
        \(beta){
          realization <- sim_sir(graph,beta,mu,seeds) # make a realization for each beta
        data.frame(beta, 
                   ninf=nrow(realization)) # create dataframe column for beta and for number infected
        })

results %>% ggplot(aes(x=beta,y=ninf))+ 
  geom_point()+
  geom_line(color="red", alpha=0.5)+
  geom_vline(xintercept = beta_c,linetype=2)+
  labs(title = "Comaparing total infections to beta values")
```

We see the trend holds. There is a very sharp increase around the
critical threshold. Note that this value is very low and the increments
between each value in the plot are also small - each representing a 0.5%
increase in the beta threshold ($\beta = 0.005$).

With such a low critical threshold, we are very likely to end up with an
epidemic in this model. Any growth is likely to lead to an epidemic.
This is seen because our critical beta value is below 1% (around
0.535%). This means that in the following questions, when we select any
1% sample, whether it is random or targeted, it will likely lead to an
epidemic.

To illustrate this, we see the basic reproduction number with 1 and 2
percent infection. The basic reproduction number (R₀) when beta is 0.02:

```{r}
mu <- 0.1  # Recovery rate

# Calculate R₀ options a
R0_1 <- (0.01 / mu) * (mean_k2 / mean_k)
R0_2 <- (0.02 / mu) * (mean_k2 / mean_k)

# print result
paste0("Basic reproduction number R₀ ≈ ", round(R0_1, 3), " with a 1% infection rate. Or, ", round(R0_2, 3), " with a 2% infection rate.")
```

R₀ \> 1 means the epidemic is likely to spread. We obtained an R₀ of
3.738, that is, very probable to spread, when the infection probability
per contact per time step is only 2%.

Seems like these rumors will be spreading in the university!

<br> <br>

### 3) Choose a β well-above above βc. Using centrality , communities or any other suitable metric, find a better set of 1% of seeds in the network so we get more infected people than the random case. Measure the difference of your choice with the random case as:

#### a) The difference in the total number of infected people

First, we set our very high β. Since our actual threshold βc is VERY low
(\<0.01), we chose a beta value that may seem low but is actually very
high relative to our value.

We select β = 0.15

We compare the random seeds to Degree centrality and PageRank to compare
a more simple measure to a more rich measure. These two should be
correlated, but we include both to see if the more rich PageRank
calculation identifies nodes which lead to more rapid spreading.

```{r}
# set the comparatively high beta and keep our same recovery rate.
beta_high <- 0.15  
mu <- 0.1
num_seeds <- ceiling(0.01 * vcount(graph))

## Collect each type of sample to compare the outcomes
# 1. Random selection (baseline)
set.seed(123)
random_seeds <- sample(1:vcount(graph), num_seeds)

# 2. Degree centrality 
degree_cent <- degree(graph)
degree_seeds <- order(degree_cent, decreasing = TRUE)[1:num_seeds]

# 3. PageRank 
pagerank_cent <- page_rank(graph)$vector
pagerank_seeds <- order(pagerank_cent, decreasing = TRUE)[1:num_seeds]

# check the betweenness measures arent' identical
identical(degree_seeds, pagerank_seeds)
```

From this, we have our 1% of central nodes (which is equal to 12 nodes
in our data) and we now compare the random selection vs targeted
selection to see how rapidly the epidemic can spread.

```{r}
## run simulations based on each of the selected and random nodes
random_sim <- sim_sir(graph, beta_high, mu, random_seeds)
degree_sim <- sim_sir(graph, beta_high, mu, degree_seeds)
pagerank_sim <- sim_sir(graph, beta_high, mu, pagerank_seeds)
```

Of our total, 1,133 people in the university network, we see that nearly
all are infected in each model. This is because of the

```{r}
# we use the unique option to ensure we have properly accounted for any duplicates (although this appears to function properly)
rand_num_inf <- length(unique(random_sim$inf))
deg_num_inf <- length(unique(degree_sim$inf))
pager_num_inf <- length(unique(pagerank_sim$inf))



```

Print all of our results for total infections. They are relatively
similar at this high value. Which is not surprising.

```{r}
cat(
  "Of the ", vcount(graph), " nodes. We see the following under each setting:\n",
  "Random selection: ", rand_num_inf, " (equal to ", round(rand_num_inf/vcount(graph)*100,2), "%)\n",
  "Degree centrality: ", deg_num_inf, " (equal to ", round(deg_num_inf/vcount(graph)*100,2), "%)\n",
    "Random selection: ", pager_num_inf, " (equal to ", round(pager_num_inf/vcount(graph)*100,2), "%)")

```

We see that at such high beta values, almost everyone is infected
regardless of the selection method. As we are above the critical
threshold, it becomes an epidemic regardless of who starts it.

#### b) The difference in the time of the peak of infection (when most infections happen).

First, aggregate the data into one table from the 3 outputs

```{r}
## line plot with both matched, or 2 plots if we show each of susceptible, infected and recovered
# add labels to merge the DF 
random_sim$type <- "Random"
degree_sim$type <- "Degree centrality"
pagerank_sim$type <- "PageRank"

results <- rbind(random_sim, degree_sim, pagerank_sim)
table(results$type)
```

We now have a long table with our 3 data points.

```{r}
## aggregate the number of infections by each selection method first then plot
results |> 
  group_by(type, t) |> 
  summarise(sum_inf = n(), .groups = 'drop') |> 
  ggplot(aes(x = t, y = sum_inf, group = type, colour = type)) +
  geom_line() +
  labs(title = "Comparison of infection trends with β=15%, targeted vs random selection",
       y = "Number of infections",
       x = "Time period",
       colour = "Selection method") + 
  theme_minimal() + # Or your preferred theme
  theme(axis.title.y = element_text(angle = 90))

```

Summary - we can show that targeted infection. I.e. those who are the
most influential in the email system being the first ones to spread the
rumor can make the rumor spread faster than if random people in the
network try to start the rumor.

PageRank is able to create a slightly faster infection peak and slightly
higher than the degree centrality measure as hypothesised. However these
2 are highly correlated.

The peaks between each network are comparable however, because we have
such a high beta value. Each selection method has a peak of around 225
infections on their biggest day of spreading. The only real difference
is **a delay in the occurrence of the peak spreading**. To illustrate
whether the peaks differ at other levels, we quickly create a model with
the beta infection rate as 3x the critical beta value, (which would be
equal to just over 1.5% and almost 10 times less infectious than the
above plot:

```{r}
## comapring with new more moderate (but still high relative to critical threshold) beta value
beta_high2 <- beta_c*3 
beta_high2
random_sim <- sim_sir(graph, beta_high2, mu, random_seeds)
degree_sim <- sim_sir(graph, beta_high2, mu, degree_seeds)
pagerank_sim <- sim_sir(graph, beta_high2, mu, pagerank_seeds)

# add labels, join and check total values. 
random_sim$type <- "Random"
degree_sim$type <- "Degree centrality"
pagerank_sim$type <- "PageRank"

results <- rbind(random_sim, degree_sim, pagerank_sim)
table(results$type) # we see the total infections are much lower. 

## now plot again to compare the trends. 
results |> 
  group_by(type, t) |> 
  summarise(sum_inf = n(), .groups = 'drop') |> 
  ggplot(aes(x = t, y = sum_inf, group = type, colour = type)) +
  geom_line() +
  labs(title = paste0("Comparison of infection trends with β=",round(beta_high2*100,2),"%, targeted vs random selection"),
       y = "Number of infections",
       x = "Time period",
       colour = "Selection method") + 
  theme_minimal() + # Or your preferred theme
  theme(axis.title.y = element_text(angle = 90))


```

Here we see the similar trend but on a much smaller scale and with more
visible noise. The Degree Centrality measure dies out almost by about
period 40. At this stage, the random assignment is just starting to find
it's peak values. Similarly, the infections drop off at the start for
our random assignment when the selected samples immediately peak due to
the importance of the infected nodes initially infected.

### 4) Using the same β, design a “quarantine strategy”: at time step t=3 or 4, quarantine of 20% the susceptible population. You can model quarantine by temporally removing these nodes. Release the quarantined nodes 8 time steps later, making them susceptible again. Measure the difference with respect to no quarantine.

We will create firstly a new function with the quarantine strategy
(based on our previous regular SIR model), where we quarantine 20% of
the susceptible population at t=4, then release them at t=12.

```{r}
sim_sir_quarantine <- function(g, beta, mu, seeds, quarantine_t = 4, release_t = 12){
  state <- rep(0, vcount(g))     # 0 = S, 1 = I, 2 = R, -1 = Quarantined
  state[seeds] <- 1              # set initial spreaders
  t <- 0
  table <- data.frame(t=0, inf=seeds)
  quarantine_group <- c()
  
  while(sum(state == 1) > 0){    # while there is still infected people
    t <- t + 1
    new_inf <- c()
    
    # quarantine 20% of susceptible nodes at time t = quarantine_t
    if(t == quarantine_t){
      susceptible_nodes <- which(state == 0)
      set.seed(t)  # reproducibility
      quarantine_group <- sample(susceptible_nodes, ceiling(0.2 * length(susceptible_nodes)))
      state[quarantine_group] <- -1  # set them as "quarantined"
    }

    # release them back at time t = release_t
    if(t == release_t && length(quarantine_group) > 0){
      state[quarantine_group] <- 0  # become susceptible again
    }

    for(i in which(state == 1)){  # loop through infected nodes
      neighbors <- neighbors(g, i)
      for(j in neighbors){
        if(state[j] == 0 && runif(1) < beta){
          new_inf <- c(new_inf, j)
        }
      }
      if(runif(1) < mu){
        state[i] <- 2  # recover
      }
    }
    
    new_inf <- unique(new_inf)  # fix: avoid duplicate infections
    state[new_inf] <- 1
    if(length(new_inf) > 0){
      table <- rbind(table, data.frame(t=t, inf=new_inf))
    }
  }
  table
}
```

Then, we will simulate 20 times with the regular SIR model and the SIR
model with quarantine policies to observe the differences.

```{r}
# set the very high beta level to same as q3
beta_high = 0.15
# Run simulation 20 times with quarantine
results_q <- replicate(20, {
  sim <- sim_sir_quarantine(graph, beta_high, mu, seeds)
  length(unique(sim$inf)) #register the total number of infected
})

# Run simulation 20 times without quarantine
results_no_q <- replicate(20, {
  sim <- sim_sir(graph, beta_high, mu, seeds)
  length(unique(sim$inf)) #register the total number of infected
})

# Compare average results
mean_q <- mean(results_q)
mean_no_q <- mean(results_no_q)
diff <- mean_no_q - mean_q

paste0("With quarantine, avg infected = ", round(mean_q), 
       "; without quarantine = ", round(mean_no_q), 
       " → Difference = ", round(diff), " nodes.")

```

We surprisingly observed that after running 20 simulations for each
strategy, the quarantine scenario resulted in a slightly **higher**
average number of infected nodes (658 vs 649) then the model without
quarantine policy, with a difference of -10 nodes.

This suggests that the quarantine intervention, as implemented, was
**not effective** in reducing spread.\

Possible reasons include:

-   Releasing quarantined susceptible nodes while the epidemic is still
    active, thereby fueling further spread;

-   Random selection of nodes to quarantine may not target critical
    spreaders;

-   Inherent stochastic variability in each simulation run.\

    A more targeted quarantine strategy (e.g., based on node centrality)
    might produce better results.

We will plot a box plot to see if this difference is significant.

```{r}
# tidy the data for later visualization
df_results <- data.frame(
  infected = c(results_no_q, results_q),
  strategy = rep(c("No Quarantine", "With Quarantine"), each = 10)
)

# Visualize the difference of results with boxplot
ggplot(df_results, aes(x = strategy, y = infected, fill = strategy)) +
  geom_boxplot(alpha = 0.5, width = 0.5, outlier.shape = NA) +
  geom_jitter(width = 0.1, size = 2, alpha = 0.7) +  # Each dot is one simulation
  labs(title = "Comparison of Final Infection Counts",
       x = "Strategy",
       y = "Total number of infected nodes") +
  theme_minimal() +
  theme(legend.position = "none")

```

The results reveal that the **median infection counts are very similar**
between the two strategies, indicating no strong difference in average
outcomes. However, the **no-quarantine group exhibits slightly more
variability**, with a few more extreme values on both ends.

Interestingly, the **quarantine strategy does not significantly reduce
infections** — in fact, it slightly increases the average count in this
particular setting. This may be due to reintroducing susceptible nodes
back into the network at a time when the infection is still active, or
due to randomly quarantining non-critical nodes.

Overall, this visualization suggests that **random quarantine of 20% of
susceptible individuals at t = 4 is not an effective intervention at
high infection rates (β = 0.15)**. Future strategies may benefit from
more targeted approaches, such as quarantining high-centrality nodes.

Apart from the final result, does the quarantine policy work at previous
stages? Before finishing this question, we would like to plot the
**trend of cumulative infected number with both policies**.

```{r}
# We run the simulation 20 times again, as previously we only obtained the final infected number, but we didn't register the infected number at each stage
set.seed(123)
runs_no_q <- map(1:20, function(i) {
  sim <- sim_sir(graph, beta, mu, seeds)
  sim$run <- i
  sim
})

runs_q <- map(1:20, function(i) {
  sim <- sim_sir_quarantine(graph, beta, mu, seeds)
  sim$run <- i
  sim
})

# Join simulation data from both policies
all_runs_no_q <- bind_rows(runs_no_q) %>% mutate(strategy = "No Quarantine")
all_runs_q <- bind_rows(runs_q) %>% mutate(strategy = "With Quarantine")

all_runs <- bind_rows(all_runs_no_q, all_runs_q)

```

Now we calculate the cumulative infected number and its confidence
interval in each stage for both policies, and then we visualize the
results.

```{r}
cumulative_all <- all_runs %>%
  group_by(strategy, run, t) %>%
  summarise(new_inf = n(), .groups = "drop") %>%
  group_by(strategy, run) %>%
  arrange(t) %>%
  mutate(cum_inf = cumsum(new_inf)) %>%
  group_by(strategy, t) %>%
  summarise(
    mean_inf = mean(cum_inf),
    sd_inf = sd(cum_inf),
    ci_upper = mean_inf + 1.96 * sd_inf / sqrt(50),
    ci_lower = mean_inf - 1.96 * sd_inf / sqrt(50),
    .groups = "drop"
  )

ggplot(cumulative_all, aes(x = t, y = mean_inf, color = strategy, fill = strategy)) +
  geom_line(size = 0.4) +
  geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), alpha = 0.3, color = NA) +
  geom_vline(xintercept = 4, linetype = "dashed", color = "darkred") +
  geom_vline(xintercept = 12, linetype = "dashed", color = "darkblue") +
  annotate("text", x = 4, y = 0, label = "Start quarantine (t=4)", angle = 90, vjust = -0.5,hjust=-1, color = "darkred", size = 3) +
  annotate("text", x = 12, y = 0, label = "End quarantine (t=12)", angle = 90, vjust = -0.5, hjust=-1, color = "darkblue", size = 3) +
  labs(title = "Infection Spread Over Time",
       subtitle = "With vs Without Quarantine (20 simulations each)",
       x = "Time step (t)",
       y = "Cumulative number of infected nodes") +
  theme_minimal() +
  theme(legend.title = element_blank())


```

This plot illustrates the average cumulative number of infected nodes
over time across 50 simulations, comparing the **No Quarantine** and
**With Quarantine** strategies, where the vertical dashed lines indicate
the start (t = 4) and end (t = 12) of the quarantine period.

We observe that both curves rise rapidly during the early time steps,
with minor differences. After t = 12, the “With Quarantine” curve begins
to diverge slightly, suggesting a delay in the epidemic's peak.

However, the final infection sizes of both strategies are very close,
and the overlapping confidence intervals indicate **no statistically
significant difference**.\

This suggests that the random quarantine of 20% susceptible individuals
has **limited impact** on the epidemic’s total spread under a high
transmission rate (β = 0.15).

In conclusion, while the quarantine flattens the curve slightly, **a
more targeted strategy may be needed** to produce meaningful reductions
in total infections.

<br>

Now we had set a relatively high beta, 15%. We also wonder if the
quarantine policy would work better while we have a **lower beta**.
Therefore, we will try a new beta 1%, which is still higher than the
critical threshold but is much lower than our previous 15%.

Here we reproduce our previous work but with beta of 1%.

```{r}
# set a lower beta
beta_high = 0.01
# Run simulation 20 times with quarantine
results_q <- replicate(20, {
  sim <- sim_sir_quarantine(graph, beta_high, mu, seeds)
  length(unique(sim$inf)) #register the total number of infected
})

# Run simulation 20 times without quarantine
results_no_q <- replicate(20, {
  sim <- sim_sir(graph, beta_high, mu, seeds)
  length(unique(sim$inf)) #register the total number of infected
})

# Compare average results
mean_q <- mean(results_q)
mean_no_q <- mean(results_no_q)
diff <- mean_no_q - mean_q

paste0("With quarantine, avg infected = ", round(mean_q), 
       "; without quarantine = ", round(mean_no_q), 
       " → Difference = ", round(diff), " nodes.")

```

```{r}
# tidy the data for later visualization
df_results <- data.frame(
  infected = c(results_no_q, results_q),
  strategy = rep(c("No Quarantine", "With Quarantine"), each = 10)
)

# Visualize the difference of results with boxplot
ggplot(df_results, aes(x = strategy, y = infected, fill = strategy)) +
  geom_boxplot(alpha = 0.5, width = 0.5, outlier.shape = NA) +
  geom_jitter(width = 0.1, size = 2, alpha = 0.7) +  # Each dot is one simulation
  labs(title = "Comparison of Final Infection Counts (beta=0.01)",
       x = "Strategy",
       y = "Total number of infected nodes") +
  theme_minimal() +
  theme(legend.position = "none")
```

```{r}
# We run the simulation 20 times again, as previously we only obtained the final infected number, but we didn't register the infected number at each stage
set.seed(123)
runs_no_q <- map(1:20, function(i) {
  sim <- sim_sir(graph, beta, mu, seeds)
  sim$run <- i
  sim
})

runs_q <- map(1:20, function(i) {
  sim <- sim_sir_quarantine(graph, beta, mu, seeds)
  sim$run <- i
  sim
})

# Join simulation data from both policies
all_runs_no_q <- bind_rows(runs_no_q) %>% mutate(strategy = "No Quarantine")
all_runs_q <- bind_rows(runs_q) %>% mutate(strategy = "With Quarantine")

all_runs <- bind_rows(all_runs_no_q, all_runs_q)
```

```{r}
cumulative_all <- all_runs %>%
  group_by(strategy, run, t) %>%
  summarise(new_inf = n(), .groups = "drop") %>%
  group_by(strategy, run) %>%
  arrange(t) %>%
  mutate(cum_inf = cumsum(new_inf)) %>%
  group_by(strategy, t) %>%
  summarise(
    mean_inf = mean(cum_inf),
    sd_inf = sd(cum_inf),
    ci_upper = mean_inf + 1.96 * sd_inf / sqrt(50),
    ci_lower = mean_inf - 1.96 * sd_inf / sqrt(50),
    .groups = "drop"
  )

ggplot(cumulative_all, aes(x = t, y = mean_inf, color = strategy, fill = strategy)) +
  geom_line(size = 0.4) +
  geom_ribbon(aes(ymin = ci_lower, ymax = ci_upper), alpha = 0.3, color = NA) +
  geom_vline(xintercept = 4, linetype = "dashed", color = "darkred") +
  geom_vline(xintercept = 12, linetype = "dashed", color = "darkblue") +
  annotate("text", x = 4, y = 0, label = "Start quarantine (t=4)", angle = 90, vjust = -0.5,hjust=-1, color = "darkred", size = 3) +
  annotate("text", x = 12, y = 0, label = "End quarantine (t=12)", angle = 90, vjust = -0.5, hjust=-1, color = "darkblue", size = 3) +
  labs(title = "Infection Spread Over Time (beta=0.01)",
       subtitle = "With vs Without Quarantine (20 simulations each)",
       x = "Time step (t)",
       y = "Cumulative number of infected nodes") +
  theme_minimal() +
  theme(legend.title = element_blank())

```

While our earlier simulations with a high infection rate (β = 0.15)
showed **minimal differences between the quarantine and no-quarantine
scenarios**, the results under a **lower infection rate (β = 0.01)**
paint a very different picture.

Surprisingly, we observed that the **average number of infections was
actually higher with quarantine** (309 infected nodes on average) than
without it (257), a difference of **-52 nodes**. This contrasts with the
previous high-β case, where the quarantine group showed only a marginal
increase (+9) in average infections.

This reversal suggests that under low-transmission conditions, the
quarantine intervention—at least in the way it is currently
implemented—**can backfire**.

Possible explanations include:

-   **Random quarantine may remove non-critical nodes**, leaving core
    transmission paths unaffected;

-   **Reintroducing susceptible nodes too early** (at t = 12) when the
    infection has not been fully contained may cause renewed spread;

-   **Stochastic effects are stronger at lower β**, meaning small
    changes can lead to large outcome variations.

The boxplot clearly illustrates this: while the no-quarantine group
still shows greater variability, the with-quarantine group registers
**higher medians** and **consistently elevated infection counts**.

Similarly, the cumulative infection curves show that the quarantine
group **catches up and overtakes** the no-quarantine group after
reintroduction of the susceptible nodes, despite an initial flattening
effect.

Taken together with our earlier findings, these results suggest that
**randomly quarantining a portion of the population may not be an
effective universal intervention**, and can even be detrimental under
certain conditions. At lower transmission rates, a poorly timed or
untargeted quarantine can lead to **worse overall outcomes**.

Future strategies should explore **data-driven, targeted quarantine**
approaches — such as removing high-centrality nodes or dynamically
adjusting quarantine duration — to achieve more reliable suppression of
spread across different epidemic scenarios.

### 5) Suppose now that you can convince 5% of people in the network not to spread that information at all.

If we have 5% who will not spread information at all, they are
effectively immune. This means we can treat the 5% of people like they
had a vaccination. Or in our situation, more like they had a realisation
that spreading rumors over the university email is bad and refuse to
read or share any gossip. In this section we compare if the targeted or
random vaccination works more effectively.

As these people refuse to spread rumours, we call them *kind_people*.

#### a) Choose those 5% randomly in the network. Simulate the SIR model above βc using 1% of the remaining nodes as seeds. Choose those seeds randomly.

First we set our random 5% of nodes to make immune. These essentially
are deleted, so we remove them from the model.

We set our infectious rate to 5% as well (beta=0.05)

```{r}
set.seed(1234)
# random vaccination campaign with 5% randomly selected - essentially delete them 
kind_people_random <- sample(1:vcount(graph),
                             vcount(graph)*0.05)
graph_rand <- delete_vertices(graph, kind_people_random)

# print our findings for info
paste0("This random selection removed ", vcount(graph)-vcount(graph_rand), " nodes, and ", ecount(graph)-ecount(graph_rand), " edges from the graph")

# now we set our 1% of seeds from the remaining nodes
inf_seeds <- sample(1:vcount(graph_rand),
                vcount(graph_rand)*0.01)

# now create our random model
realization_random <- sim_sir(graph_rand,
                              beta = beta_c,
                              mu,
                              inf_seeds) %>%
  group_by(t) %>% summarize(ninf=n())

realization_random |> 
  ggplot(aes(x = t, y = ninf)) +
  geom_line() +
  labs(title = paste0("Infection trend with randomly assigned nice people (non gossipers)"),
       y = "Number of infections",
       x = "Time period") + 
  theme_minimal() + 
  theme(axis.title.y = element_text(angle = 90))

```

#### b) Choose those 5% according to their centrality. Simulate the SIR model βc above using 1% of the remaining nodes as seeds. Choose those seeds randomly.

We will use PageRank to select the 5% as this had the most extreme peak
in Q3.

```{r}
# targeted selection of most influential  5% randomly selected - essentially delete them, using our exiting pagerank_cent calculations
num_seeds <- vcount(graph)*0.05
kind_people_pager <- order(pagerank_cent, decreasing = TRUE)[1:num_seeds]

graph_pager <- delete_vertices(graph, kind_people_pager)

# print our findings for info
paste0("This pagerank selection removed ", vcount(graph)-vcount(graph_pager), " nodes, and ", ecount(graph)-ecount(graph_pager), " edges from the graph")

# now we set our 1% of seeds from the remaining nodes
inf_seeds <- sample(1:vcount(graph_pager),
                vcount(graph_pager)*0.01)

# now create our random model
realization_pager <- sim_sir(graph_pager,
                              beta = 0.05,
                              mu,
                              inf_seeds) %>%
  group_by(t) %>% summarize(ninf=n())

#plot
realization_pager |> 
  ggplot(aes(x = t, y = ninf)) +
  geom_line() +
  labs(title = paste0("Infection trend with targeted nice people (non gossipers)"),
       y = "Number of infections",
       x = "Time period") + 
  theme_minimal() + 
  theme(axis.title.y = element_text(angle = 90))

```

#### c) Measure the difference between both cases as you did in step 3.

### 6) Comment on the relationship between the findings in steps 3 and 5 using the same type of centrality for the 1% in step 3 and 5% in step 5.

### 7) With the results of step 2, train a model that predicts that time to infection of a node using their degree, centrality, betweeness, page rank and any other predictors you see fit. Use that model to select the seed nodes as those with the smallest time to infection in step 3. Repeat step 5 with this knowledge.
