---
title: "University Email Network"
author: "Yijia Lin, Bradley McKenzie and Diego Paroli"
date: "`r Sys.Date()`"
output: 
  rmdformats::html_clean:
    lightbox: false
    thumbnails: false
    toc: yes
    toc_float: true
    toc_depth: 3
---

```{r,include=F}
knitr::opts_chunk$set(cache=T,message=FALSE,warning=FALSE)
options(scipen=700)
```


## Libraries

```{r, message=FALSE, warning=FALSE}
rm(list = ls())
library(tidyverse)
library(igraph)
library(tidygraph)
library(ggraph)
library(ggthemes)
library(here)
library(caret)
```

## Read the data

```{r, message=FALSE, warning=FALSE}
nodes <- read_csv(here("network_data", "nodes.csv"))
links <- read_csv(here("network_data", "edges.csv"))
```

## Description of the dataset

This network is a directed and unweighted social communication network, which represents the exchange of emails among members of the Rovira i Virgili University in Spain, in 2003.

Source: [Netzschleuder](https://networks.skewed.de/net/uni_email).

#### Properties

Directed, unweighted, social communication network.

#### Nodes and links

```{r}
head(nodes)
head(links)
```

Within the `nodes` dataframe, the column `# index` is the one indicating the index of the nodes, the column `name` would contain the corresponding name of each node, but they have been anonymized for privacy reason so this column is still a series of numbers. The `_pos` column represents the coordinates of each node in a 2D space, used for visualization or layout in graph-related tasks. 

In the `links` dataframe, the column `# source` and `target` indicate the indexes of the 2 nodes forming a link.

#### Graph

```{r}
graph <- graph_from_data_frame(links, directed = TRUE, vertices = nodes)
graph
```

## Initial graph exploration

To understand the graph, we can plot the degree distribution to check whether we have the expected power-law distribution or another type. This will help to interpret the findings later when we remove links.

First, we check for any loops, these would be emails sent to self. And if so, we can simplify the graph to remove these (and also any eventual multiple edge). 

```{r}
paste0("The number of recursive (self-directed) emails in the network is: ", 
       ecount(graph) - ecount(simplify(graph)))

# we see that one exists, so we simplify the graph to remove it. 
graph <- simplify(graph)

# check our graph is setup properly
is_igraph(graph)
```

Next, calculate basic descriptive statistics of the plot:

```{r}
paste0("There are ", vcount(graph), " nodes and ", ecount(graph), " edges in our Spanish email network")

# check if our graph is fully connected
is_connected(graph)

#degree
degree_all <- degree(graph, mode="all")
degree_out <- degree(graph, mode="out")
paste0("The average degree is: ", round(mean(degree(graph)), 2), " when we DO NOT consider the direction of the emails.")
paste0("The average degree is: ", round(sum(degree_out)/vcount(graph), 2), " when we DO consider the emails to be directed outward from the node (sender) to a recipient")

# most connected node:
paste0("The most connected node is number ", which.max(degree(graph))[1], " which has ", max(degree(graph)), " links.")
```

For the purposes of this homework, we are not factoring in this directional property of the graph. We are considering that the emails are indicating a social relationship between the two nodes (sender and receiver) and it is irrelevant who sent the email. This means the average interactions for each individual node is 19.24 (which would represent email interactions with 19.24 different people on average).

Now check the degree distribution;

```{r, fig.width=8}
# set average node for label 
avg_deg <- round(mean(degree(graph)),2)

# plot linear distribution
ggplot() + 
  geom_histogram(aes(x = degree_all), bins=40) + 
  geom_vline(aes(xintercept = avg_deg, colour = "red"), show.legend = FALSE) +
  annotate("text", x = avg_deg+5, y = 160, label = paste0("Avg deg = ", avg_deg), angle = 90, color = "red") + 
  labs(title = "Degree distribution histogram for Uni emails",
       x = "Degrees",
       y = "Number of nodes") +
  theme(axis.title.y = element_text(angle = 90))
```

We see the common power law distribution. There are few very connected nodes and many less connected nodes. We see that the max node must be node number 105 we reported earlier, with its 142 links. This is far more than the second most connected.

Visualization of the network:

```{r, fig.width=8}
# set colour and size only for the top 50 nodes
top_50_nodes <- order(degree_all, decreasing = TRUE)[1:50]
V(graph)$color <- "grey3"
V(graph)[top_50_nodes]$color <- "red"
V(graph)$size <- 0.75
V(graph)[top_50_nodes]$size <- 2

# plot the full network
ggraph(graph, layout = "kk") +
  geom_edge_link(width = 0.1, alpha = 0.5, color = "grey") +  
  geom_node_point(color = V(graph)$color, size = V(graph)$size) +  
  theme_void() +  
  ggtitle("Visualisation of the network, 50 most connected nodes highlighted")+
  theme(legend.position = "none")
```

We see that the most connected nodes are all centered in the graph. On the outside, we can see the nodes with only a couple of connections.


## Steps

### 1) Create table with positive and negative links 

#### Task 1: Delete a fraction of real edges in the network and create a table of those links deleted (positive class) and of links non-present (negative class)

First we'll create the positive class: existing edges that are removed from the network.

```{r}
# Set seed for reproducibility
set.seed(123)

# Randomly delete some real edges
edge_list <- as_data_frame(graph, what = "edges")
n_edges <- nrow(edge_list)
n_remove <- floor(0.1 * n_edges) # We will delete 10% of the real edges
removed_edges <- edge_list[sample(1:n_edges, n_remove), ]
# Formatting appropriately the edges to pass them as argument to delete_edges
edge_strings <- paste(removed_edges$from, 
                      removed_edges$to, 
                      sep = "|")
graph_modified <- delete_edges(graph, edges = edge_strings)

# Label the deleted real edges as positive class (1)
positive_edges <- removed_edges %>%
  mutate(label = 1)
```

We'll now focus on the negative class: edges that do not exist in the network.

We will be selecting false edges (non-existent links) only among high-degree nodes. This is done in order to select false edges that could at least be plausible and thus help us more in the training process. Low degree nodes would be unlikely to form an edge any way and so using those non-existent edges between low degree nodes would not add any information in our model.

```{r}
# Create a new data frame to register the negative class (non-existent links)
false_edges <- data.frame()

# Filter for more connected nodes
most_connected <- which(degree(graph)>15)
# We choose 15 arbitrarily (a number slightly below our average degree)

set.seed(123)

# Create a loop for creating made-up links, registering all in `false_edges`
while (nrow(false_edges) < n_remove) {
  node1 <- sample(most_connected, 1)
  node2 <- sample(most_connected, 1)
  
  # avoid self-connecting
  if (node1 == node2) next
  
  # Check if two nodes are connected, if not, create a negative link for them
  if (!are_adjacent(graph, node1, node2)) {
    edge_row <- data.frame(from = as_ids(V(graph)[node1]), 
                           to = as_ids(V(graph)[node2]),
                           label = 0)
    # Function to check to avoid adding same link
    if (!any(apply(false_edges, 1, function(row) all(row == edge_row)))) {
      false_edges <- rbind(false_edges, edge_row)
    }
  }
}
```

```{r}
# standardise the format of tables before binding
positive_edges <- positive_edges %>%
  mutate(from = as.numeric(from), to = as.numeric(to))

false_edges <- false_edges %>%
  mutate(from = as.numeric(from), to = as.numeric(to))

# Bind the positive and negative edges
all_edges <- bind_rows(positive_edges, false_edges)
head(all_edges)
table(all_edges$label)
```

### 2) Calculate heuristics for each class

#### Task 2: Generate a number of proximity/similarty metrics heuristics for each link in the positive and negative class

We'll create a function to compute different proximity/similarity metrics heuristics to be used as features in the prediction model later.

```{r}
compute_heuristics <- function(graph, edge_df) {
  edge_df <- edge_df %>%
    mutate(
      # Calculate common neighbors using the neighbors function
      common_neighbors = map2_int(from, to, ~ 
        length(intersect(neighbors(graph, .x, mode = "all"), 
                         neighbors(graph, .y, mode = "all")))),
      
      # Calculate Jaccard coefficient
      # We don't use the similarity.jaccard function as it's difficult to handle a matrix
      jaccard = map2_dbl(from, to, ~ {
        nei_x <- neighbors(graph, .x, mode = "all")
        nei_y <- neighbors(graph, .y, mode = "all")
        length(intersect(nei_x, nei_y)) / length(union(nei_x, nei_y))
      }),

      # Calculate Adamic-Adar metric
      # We don't use the similarity.invlogweighted function as it's difficult to handle a matrix
      adamic_adar = map2_dbl(from, to, ~ {
        nei_common <- intersect(neighbors(graph, .x, mode = "all"), 
                                neighbors(graph, .y, mode = "all"))
        if (length(nei_common) == 0) return(0)
        degrees <- degree(graph, nei_common, mode = "all")
        sum(1 / log(degrees))
      }),
      
      # Calculate preferential attachment by multiplying the degrees
      pref_attachment = map2_dbl(from, to, ~ 
        degree(graph, .x, mode = "all") * degree(graph, .y, mode = "all")),
      
    )
  return(edge_df)
}
```

We'll now apply the created function and compute all proximity/similarity metrics heuristics that we learned in class on our dataset.

```{r}
all_edges_with_heuristics <- compute_heuristics(graph_modified, all_edges)
# view the true edges
all_edges_with_heuristics |> filter(label == 1) |> head()
# view the fake edges
all_edges_with_heuristics |> filter(label == 0) |> head()

all_edges_with_heuristics |>
  summarise(across(c(common_neighbors, jaccard, adamic_adar, 
                     pref_attachment), 
                   mean, .names = "mean_{.col}"), 
            .by = label)
```

We see that the in the real edges, the average of each heuristic tends to be higher than for the fake edges. This is expected as the heuristics are designed to measure the likelihood of a link existing between two nodes and a higher score for them represents nodes which are more likely to be proximate.

We tried to use also the shortest path (distance) between two nodes as a feature in our model, however this heuristic cannot be used because it can take infinite values in the case that one of the removed edges causes part of the graph to disconnect.

### 3) Predict the links using CV

####  Task 3: Train a binary classifier to predict the links, i.e., to predict the class (positive/negative) using those heuristics. Use crossvalidation.

First we split in test and training datasets:

```{r}
# Create training (80%) and test set (20%)
set.seed(123) 
train_indices <- sample(1:nrow(all_edges_with_heuristics), size = 0.8 * nrow(all_edges_with_heuristics))
train_edges <- all_edges_with_heuristics[train_indices, ]
test_edges <- all_edges_with_heuristics[-train_indices, ]

# Convert the labels into factors, for the logit model
train_edges$label <- as.factor(train_edges$label)
```

We'll now build a logit model (binomial family from `glm` package) to predict if a link exists (1) or not (0) using the different proximity/similarity metrics heuristics created previously as features in the model.

```{r}
# Use cross validation to test the performance
set.seed(123)
train_control <- trainControl(method = "cv", number = 10)
model_cv <- train(label ~ common_neighbors + jaccard + adamic_adar + pref_attachment,
                  data = train_edges, 
                  method = "glm", 
                  family = binomial, 
                  trControl = train_control,
                  metric = "Accuracy")
```

Finally we predict the test set. 

```{r}
# Predict the testing set
predictions <- predict(model_cv, newdata = test_edges, type = "prob")
# Probabilities of belonging to a class or the other
head(predictions)
```

### 4) Evaluate model performance

#### Step 4: Evaluate the precision of the model. Which heuristic is the most important. Why do you think it is the most important?

Let's first look at the summary of the model.

```{r}
summary(model_cv)
```

The most significant heuristic is Adamic-Adar (in terms of lowest p-value), followed by Jaccard and then preferential attachment and common neighbors. 

Looking at the magnitude of the coefficients could give us an hint on which of the heuristic is the most important. However, we must take into account that the heuristic measure do not use the same scale, so we will be running another model in which we standardize the features of the model

```{r}
set.seed(123)
train_control <- trainControl(method = "cv", number = 10)
model_cv <- train(label ~ common_neighbors + jaccard + adamic_adar + pref_attachment,
                  data = train_edges, 
                  method = "glm", family = binomial, 
                  trControl = train_control,
                  # apply standardization
                  preProcess = c("center", "scale"), 
                  metric = "Accuracy")
summary(model_cv)
```

Adamic-Adar in addition to being the most significant, is also the heuristic with the biggest coefficient. 

Adamic-Adar is a score that takes into account common neighbors of two nodes, but it also takes into account the degree of the common neighbors. This means that it gives more weight to common neighbors with lower degrees, which is interpreted as a good indicator of the likelihood of a link forming. On the opposite the Jaccard's coefficient does not do that, as it only calculates the proportion of neighbors in common, nor does it do it the common neighbors which simply counts the common neighbors. This particular feature of Adamic-Adar which discounts whether common neighbors have low or high degree is probably what makes it the most important heuristic in our model. 

Furthermore it is possible that given the power-law like degree distribution of our network this feature is potentially even more important. Many people are probably connected to the hubs of the network (which we can interpret as faculty or administrative people from the university), but if two people share connection with those hubs (ex. a professor that teaches many courses), it is not a good proxy that they are connected to each other. On the other hand if two people are connected to many common neighbors that are not hub (ex. students), it is likely that they might be in the same course and thus this is a good proxy that they are connected to each other.

Overall, we determine that Adamic-Adar is the most important heuristic because it weights less common relationships, which matches the real world experience we would see in a university sample.  

The negative coefficients of the common neighbors heuristic and preferential attachment (which tries to capture the fact that nodes with higher degree are more likely to receive new connections), are of difficult interpretation as we would expect those coefficients to be positive as well. One possible explanation is multicollinearity. We are going to check if our heuristics are very correlated to each other:

```{r}
cor(train_edges[, c("common_neighbors", "jaccard", "adamic_adar", "pref_attachment")])
```

We can see the correlation between `common_neighbors` and `adamic_adar` is extremely high (0.99), indicating. `common_neighbors` and `jaccard` are also strongly correlated (0.82). Also `adamic_adar` and `jaccard` share a very high correlation (0.84). `shortest_path` is the only one that shows low correlation with others. These extremely high values of correlation make coefficient interpretation difficult at all levels, undermining our previous strategy to identify the most important heuristic. Nonetheless, our theoretical explanation of why Adamic-Adar is the most important heuristic still holds.  

We'll now evaluate the model performance:

```{r}
# Confusion matrix
confusion_matrix <- confusionMatrix(as.factor(ifelse(predictions[, 2] > 0.5, 1, 0)), 
                                     as.factor(test_edges$label), positive = "1")
confusion_matrix
```

Overall accuracy is 78%, specificity is higher (89%), while sensitivity is lower (67%). This means, as can be seen from the confusion matrix, that the model is better at identifying negative links (those that never existed), than positive links (those that were actually present). The model often labels as non-existent link, links that actually were there.

### 5) Potential improvements 

#### Task 5: Comment on potential ways to improve the link prediction

**1. Accessing additional information**

The best way to improve model prediction is by including additional information on the students. This could be broken down into information on the nodes or edges. 

_Improved node information_ could include details on the type of university person they are. For example, are they a student or a professor or administrative staff. For students and professors, information on the faculty and subjects they take. Some sociodemographic information could help but it is less useful than university information. 

_Edge information_ could also improve prediction. For example, the number of emails sent between the individuals to indicate strength of the relationship. The number of emails could then be included as a weight in the edge when doing the prediction models. 

One potential solution to improve the prediction with no information is to perform _unsupervised learning to generate clusters_ (aka communities for network analysis). With no targets, we can try to identify different neighbourhoods and assign values from the clustering. With the very basic information available, this is the only option to manually add on new columns of potentially identifiable data. The major limitation with this is that we wouldn't be able to understand why the clusters have formed and whether they are logical. 


**2. Try make more localised predictions**

Prediction is always difficult with class imbalance in a dataset. In any network we do have quite extreme class imbalance overall. Oversampling of the true linkages is a basic way the prediction of positive cases can be improved. 

The model is trying to predict individual links. However, the average node has a degree of 19.24 links, while there are 1,133 nodes. That means only 1.7% of all possible edges exist. If the nodes were randomly assigned, that would mean there is a 1.7% chance that two nodes are connected together.

When we take random samples of the missing nodes, if we were trying to predict the entire graph, the predictive model would just predict 0 links. And it would have 98.3% accuracy because that is the majority class. This highlights why the important prediction variable for us is the true positive estimations. 

We solved this in our predictions because we have taken a balanced sample of true and false links. However, one potential further improvement to this would be doing more localised predictions. Hypothetically, if we produced clusters/communities, and we treated each cluster as an ego graph, then making predictions within each of the clusters, the average degree relative to the number of nodes would increase and we would expect to see better predictive models locally. 


**3. Try further ML model optimisation with the heuristics**

A more complex model could include using some regularization (Lasso/Ridge penalities) to tackle multicollinearity. This would likely be a smaller improvement, but optimising the heuristics and testing different combinations could optimise the model. 

Calculating even more complex heuristic could also help in adding useful information which could improve predictions. 

In a more basic change, the threshold for where a link is identified could be altered to find the optimal point on the ROC curve to find the best threshold and the best balance between sensitivity and specificity. 

Potentially also sampling fake edges between nodes with even higher degree (>15 which was our threshold) could help. This could especially be useful, in light of the fact that our model struggles more in identifying positive (existing links). Therefore adding more information on even more plausible links (between nodes with degree substantially higher than average) could help the model better differentiate and identify actual links from non-exisiting links.

We'll try to do this below, where all our pipeline is copied and pasted, only changing the threshold to sample fake edges.

```{r}
false_edges <- data.frame()

# We now choose 30 doubling from the previous threshold which was 15
most_connected <- which(degree(graph)>30)

set.seed(123)
while (nrow(false_edges) < n_remove) {
  node1 <- sample(most_connected, 1)
  node2 <- sample(most_connected, 1)
  
  # avoid self-connecting
  if (node1 == node2) next
  
  # Check if two nodes are connected, if not, create a negative link for them
  if (!are_adjacent(graph, node1, node2)) {
    edge_row <- data.frame(from = as_ids(V(graph)[node1]), 
                           to = as_ids(V(graph)[node2]),
                           label = 0)
    # Function to check to avoid adding same link
    if (!any(apply(false_edges, 1, function(row) all(row == edge_row)))) {
      false_edges <- rbind(false_edges, edge_row)
    }
  }
}

positive_edges <- positive_edges %>%
  mutate(from = as.numeric(from), to = as.numeric(to))
false_edges <- false_edges %>%
  mutate(from = as.numeric(from), to = as.numeric(to))
all_edges <- bind_rows(positive_edges, false_edges)

all_edges_with_heuristics <- compute_heuristics(graph_modified, all_edges)

set.seed(123) 
train_indices <- sample(1:nrow(all_edges_with_heuristics), size = 0.8 * nrow(all_edges_with_heuristics))
train_edges <- all_edges_with_heuristics[train_indices, ]
test_edges <- all_edges_with_heuristics[-train_indices, ]

train_edges$label <- as.factor(train_edges$label)

set.seed(123)
train_control <- trainControl(method = "cv", number = 10)
model_cv <- train(label ~ common_neighbors + jaccard + adamic_adar + pref_attachment,
                  data = train_edges, 
                  method = "glm", 
                  family = binomial, 
                  trControl = train_control,
                  metric = "Accuracy")
summary(model_cv)

predictions <- predict(model_cv, newdata = test_edges, type = "prob")

confusion_matrix <- confusionMatrix(as.factor(ifelse(predictions[, 2] > 0.5, 1, 0)), 
                                     as.factor(test_edges$label), positive = "1")
confusion_matrix
```

It seems that this has indeed helped our predictions as accuracy has increased, but most importantly we were able to achieve a higher sensitivity. This could just be out of luck of sampling meaningful non-existent links of course, but nonetheless our results seem robust. 

It is also interesting to see how the model changes, it seems that preferential attachment plays now a bigger role at the expense of Jaccard and common neighbors. In light of this we can also probably understand better its negative coefficient: given that we sample fake edges from high degree nodes, the preferential attachment of the fake links is likely to be high and thus the model could interpret a high preferential attachment as higher probability of that edge being fake. This goes against the theoretical concept that preferential attachment was developed to capture, but nevertheless it seems that it is helping our predictions.
